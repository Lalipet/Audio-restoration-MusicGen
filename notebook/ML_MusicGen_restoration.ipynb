{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Setup\n",
        "\n",
        "This cell installs the necessary libraries, loads the pre-trained MusicGen small model (https://huggingface.co/facebook/musicgen-small), and initializes the generation pipeline. MusicGen generates music from text descriptions by converting text into audio tokens, predicting them using a transformer, and decoding them into 32kHz audio waveforms.\n"
      ],
      "metadata": {
        "id": "wd1sr59X9W8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "\n",
        "from scipy.signal import butter, sosfilt\n",
        "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
        "from IPython.display import Audio, display"
      ],
      "metadata": {
        "id": "K8373O8qFM7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_id = \"facebook/musicgen-small\"\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "model = MusicgenForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(\"Ready on:\", device)\n"
      ],
      "metadata": {
        "id": "egFs7Q0503X0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Music Generation\n",
        "\n",
        "We define a set of text prompts for different music genres and randomly select one for each genre. Then we use the MusicGen model to generate audio for each selected prompt. The generated audio is saved as WAV files.\n"
      ],
      "metadata": {
        "id": "j6RTQcY6FTT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genre_prompts = {\n",
        "    \"pop\": [\n",
        "        \"Upbeat pop track with bright synths\",\n",
        "        \"Emotional pop ballad with piano, strings and soft drums\",\n",
        "        \"Modern pop song with  groovy bass\",\n",
        "        \"Dance-pop track with four-on-the-floor kick and sparkling keys\",\n",
        "        \"Mid-tempo pop track with airy pads and mellow vocal hooks\"\n",
        "    ],\n",
        "    \"rock\": [\n",
        "        \"Energetic rock track with distorted guitars and powerful drums\",\n",
        "        \"Classic rock riff with electric guitar and steady bass\",\n",
        "        \"Slow rock ballad with clean guitar and emotional solos\",\n",
        "        \"Indie rock track with jangly guitars and tight drum groove\",\n",
        "        \"Heavy rock track with aggressive rhythm guitars\"\n",
        "    ],\n",
        "    \"jazz\": [\n",
        "        \"Jazz quartet with saxophone, double bass, drums and warm piano\",\n",
        "        \"Smooth jazz track with electric piano and soft saxophone melody\",\n",
        "        \"Jazz fusion track with syncopated drums and fretless bass\",\n",
        "        \"Big band swing with brass section, walking bass and fast ride cymbal\",\n",
        "        \"Atmospheric jazz with trumpet and piano\"\n",
        "    ],\n",
        "    \"classical\": [\n",
        "        \"Romantic classical piece for solo piano with expressive dynamics\",\n",
        "        \"String quartet playing a lyrical, slow movement\",\n",
        "        \"Full symphony orchestra with sweeping strings and bold brass\",\n",
        "        \"Baroque-style piece with harpsichord and chamber ensemble\",\n",
        "        \"Soft classical piece for piano and cello duet\"\n",
        "    ],\n",
        "    \"hiphop\": [\n",
        "        \"Hip-hop beat with dusty drums and vinyl crackle\",\n",
        "        \"Modern trap beat with deep bass and rapid hi-hats\",\n",
        "        \"Lo-fi hip-hop beat with warm piano samples and laid-back groove\",\n",
        "        \"Aggressive hip-hop instrumental with heavy kicks and brass stabs\",\n",
        "        \"Chill hip-hop beat with soft Rhodes chords and subtle percussion\"\n",
        "    ],\n",
        "    \"electronic\": [\n",
        "        \"Ambient electronic soundscape with evolving pads and soft pulses\",\n",
        "        \"Melodic techno track with steady kick and hypnotic arpeggios\",\n",
        "        \"Deep house groove with warm bass and airy chords\",\n",
        "        \"Dubstep-style track with bass and sharp snares\",\n",
        "        \"Future bass track with detuned synth chords and sidechain pumping\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "selected_prompts = {}\n",
        "for genre, prompt_list in genre_prompts.items():\n",
        "    chosen = random.choice(prompt_list)\n",
        "    selected_prompts[genre] = chosen\n",
        "\n",
        "\n",
        "print(\"Chosen prompts:\")\n",
        "for genre, prompt in selected_prompts.items():\n",
        "    print(f\"- {genre}: {prompt}\")\n",
        "\n",
        "duration_tokens = 600   #10-12 seconds\n",
        "sr = model.config.audio_encoder.sampling_rate\n",
        "print(f\"Sampling rate: {sr} Hz\")\n",
        "\n",
        "for genre, prompt in selected_prompts.items():\n",
        "    print(f\"\\nGeneration for '{genre}'...\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[prompt],\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.inference_mode(), torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=(device == \"cuda\")):\n",
        "        audio_values = model.generate(\n",
        "            **inputs,\n",
        "            do_sample=True,\n",
        "            guidance_scale=2.5,\n",
        "            max_new_tokens=duration_tokens,\n",
        "            temperature=1.0,\n",
        "            top_k=250,\n",
        "            top_p=0.95,\n",
        "        )\n",
        "\n",
        "    #(batch, channels, samples)\n",
        "    audio = audio_values[0].detach().cpu().float().numpy()\n",
        "\n",
        "    display(Audio(audio, rate=sr))\n",
        "\n",
        "    filename = f\"raw_{genre}.wav\"\n",
        "    sf.write(filename, audio.T, sr)\n",
        "    print(f\"Saved as {filename}\")"
      ],
      "metadata": {
        "id": "kvU1xaIq9y8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing: Normalization and High-Pass Filtering\n",
        "Through this function we normalize the amplitude of each generated audio file and apply a high-pass filter to remove low frequencies (below 30 Hz by default).\n"
      ],
      "metadata": {
        "id": "f_sLsyrJivPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_normalize_highpass(audio, sr, hp_cutoff=30.0):\n",
        "    audio = audio.astype(np.float32)\n",
        "\n",
        "    # stereo to mono\n",
        "    if audio.ndim == 2:\n",
        "        audio = np.mean(audio, axis=1)\n",
        "\n",
        "    # peak normalization\n",
        "    peak = np.max(np.abs(audio))\n",
        "    if peak > 0:\n",
        "        audio = 0.99 * audio / peak\n",
        "\n",
        "    sos_hp = butter(\n",
        "        N=2,\n",
        "        Wn=hp_cutoff,\n",
        "        btype=\"highpass\",\n",
        "        fs=sr,\n",
        "        output=\"sos\"\n",
        "    )\n",
        "\n",
        "    # high-pass filtering\n",
        "    audio_hp = sosfilt(sos_hp, audio)\n",
        "    return audio_hp\n",
        "\n",
        "genres = [\"pop\", \"rock\", \"jazz\", \"classical\", \"hiphop\", \"electronic\"]\n",
        "\n",
        "for genre in genres:\n",
        "    raw_filename = f\"raw_{genre}.wav\"\n",
        "    out_filename = f\"preproc_{genre}.wav\"\n",
        "\n",
        "    audio, sr = sf.read(raw_filename)\n",
        "    audio_pre = preprocess_normalize_highpass(audio, sr, hp_cutoff=30.0)\n",
        "\n",
        "    sf.write(out_filename, audio_pre, sr)\n",
        "    print(f\"Preprocess per {genre}: saved as {out_filename}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NQB0m9fQ9iVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Denoising: noise reduction with Noisereduce library\n",
        "We reduce background noise from each preprocessed audio clip using the noisereduce library. The function estimates a noise profile, and applies non-stationary noise reduction.\n"
      ],
      "metadata": {
        "id": "eM3wVlc3ucDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q noisereduce\n",
        "import noisereduce as nr\n",
        "\n",
        "def denoise_noisereduce_mono(audio, sr, noise_duration=0.1):\n",
        "    audio = audio.astype(np.float32)\n",
        "\n",
        "    if audio.ndim == 2:\n",
        "        audio = np.mean(audio, axis=1)\n",
        "\n",
        "    n_noise = int(noise_duration * sr)  # samples\n",
        "    n_noise = min(n_noise, len(audio) // 2)  # if clip is too short it takes its half instead of noise_duration\n",
        "    y_noise = audio[:n_noise]\n",
        "\n",
        "    y_denoised = nr.reduce_noise(\n",
        "        y=audio,\n",
        "        y_noise=None,\n",
        "        sr=sr,\n",
        "        prop_decrease=0.7,  # noise reduction percentage\n",
        "        stationary=False    # assuming non-stationary noise\n",
        "    )\n",
        "\n",
        "    return y_denoised\n",
        "\n",
        "for genre in genres:\n",
        "\n",
        "    pre_file = f\"preproc_{genre}.wav\"\n",
        "    out_file = f\"denoised_{genre}.wav\"\n",
        "\n",
        "    print(f\"\\nGenre: {genre}\")\n",
        "\n",
        "    audio_pre, sr = sf.read(pre_file)\n",
        "    print(\"Preprocessed:\")\n",
        "    display(Audio(audio_pre, rate=sr))\n",
        "\n",
        "    audio_denoised = denoise_noisereduce_mono(audio_pre, sr)\n",
        "    print(\"Denoised:\")\n",
        "    display(Audio(audio_denoised, rate=sr))\n",
        "\n",
        "    # Save denoised file\n",
        "    sf.write(out_file, audio_denoised, sr)\n",
        "    print(f\"Saved as {out_file}\")"
      ],
      "metadata": {
        "id": "0cGIFppUHPHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "def plot_waveform_and_spec(y, sr, title):\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    plt.title(title)\n",
        "    librosa.display.waveshow(y, sr=sr)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "for genre in genres:\n",
        "\n",
        "    audio_pre, sr = sf.read(f\"preproc_{genre}.wav\")\n",
        "    audio_denoised, sr = sf.read(f\"denoised_{genre}.wav\")\n",
        "\n",
        "    plot_waveform_and_spec(audio_pre, sr, f\"{genre} - Preprocessed\")\n",
        "    plot_waveform_and_spec(audio_denoised, sr, f\"{genre} - Denoised\")\n"
      ],
      "metadata": {
        "id": "UVizkjjRgvOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bandwidth Extension with HiFi‑GAN BWE (from 24 kHz to 48 kHz)\n",
        "\n",
        "We use a bandwidth‑extension model (HiFi‑GAN BWE, https://github.com/brentspell/hifi-gan-bwe) (which is a third‑party implementation and not the official HiFi‑GAN release) to enhance the bandwidth of the denoised\n",
        "audio clips.  \n",
        "Each cleaned audio file (assumed to be mono or converted to mono) is downsampled to the 24 kHz input rate expected by the model, and feed it to the BWE model to generate a 48 kHz version with reconstructed high‑frequency content.\n",
        "\n",
        "If the BWE model fails for any reason, a standard resampling fallback  is applied to produce a 48 kHz output.  \n",
        "\n",
        "This model is optimized for speech-like audio and it is a generative model that reconstructs high-frequency content lost in low-resolution signals.  \n"
      ],
      "metadata": {
        "id": "WUbuCQlJ57w-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q hifi-gan-bwe\n",
        "import warnings\n",
        "import torchaudio\n",
        "import os\n",
        "from hifi_gan_bwe import BandwidthExtender\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"starting BWE...\")\n",
        "\n",
        "# Setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if 'bwe_model' not in locals():\n",
        "    bwe_model = BandwidthExtender.from_pretrained(\"hifi-gan-bwe-13-59f00ca-vctk-24kHz-48kHz\").to(device)\n",
        "    bwe_model.eval()\n",
        "\n",
        "target_sr_out = 48000\n",
        "fs_in = 24000\n",
        "\n",
        "for genre in genres:\n",
        "    in_path = f\"denoised_{genre}.wav\"\n",
        "    out_path = f\"bwe_{genre}_48k.wav\"\n",
        "\n",
        "    if not os.path.exists(in_path):\n",
        "        print(f\"Skipping {genre}: file not found.\")\n",
        "        continue\n",
        "\n",
        "    y, sr = sf.read(in_path, dtype=\"float32\")\n",
        "\n",
        "    if y.ndim == 2:\n",
        "        y = np.mean(y, axis=1)\n",
        "\n",
        "    x = torch.from_numpy(y).float().unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "    # Resampling\n",
        "    if sr != fs_in:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=fs_in).to(device)\n",
        "        x = resampler(x)\n",
        "        current_sr = fs_in\n",
        "    else:\n",
        "        current_sr = sr\n",
        "\n",
        "    print(f\"Processing {genre}...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            y_48k_t = bwe_model(x, current_sr)\n",
        "            y_48k = y_48k_t.squeeze().cpu().numpy()\n",
        "\n",
        "            print(f\"{genre}: BWE Success\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{genre}: BWE Failed. Error: {e}\")\n",
        "            print(f\"   -> Fallback: standard resampling\")\n",
        "\n",
        "            y_48k_t = torchaudio.functional.resample(x, orig_freq=current_sr, new_freq=target_sr_out)\n",
        "            y_48k = y_48k_t.squeeze().cpu().numpy()\n",
        "\n",
        "    sf.write(out_path, y_48k, target_sr_out)\n",
        "\n",
        "    print(f\"Preview {genre}:\")\n",
        "    display(Audio(y_48k, rate=target_sr_out))"
      ],
      "metadata": {
        "id": "p8iPYTb3Nkz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio Quality evaluation at Each Pipeline Stage\n",
        "\n",
        "We evaluate the effect of each stage in the audio pipeline on audio quality.\n",
        "\n",
        "Two key metrics are computed:\n",
        "- Log Spectral Distance (LSD): compares the spectral difference between each stage and the original raw audio (resampled to 16 kHz), measuring signal degradation.\n",
        "\n",
        "- High-Frequency Energy Ratio (HF-ratio): measures the proportion of energy above 8 kHz in the native sampling rate of each audio file, useful to quantify how much high-frequency content is preserved or added.\n"
      ],
      "metadata": {
        "id": "rx1Z2Q7EmCjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q librosa pandas\n",
        "\n",
        "import librosa, pandas as pd\n",
        "\n",
        "def load_mono(path, target_sr=None):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"File non trovato: {path}\")\n",
        "\n",
        "    audio, sr = sf.read(path, dtype=\"float32\")\n",
        "    if audio.ndim == 2:\n",
        "        audio = np.mean(audio, axis=1)\n",
        "    if target_sr is not None and sr != target_sr:\n",
        "        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
        "        sr = target_sr\n",
        "    return audio, sr\n",
        "\n",
        "def normalize(x):\n",
        "    return x / (np.max(np.abs(x)) + 1e-12)\n",
        "\n",
        "# same dimension for each input\n",
        "def align_signals(ref, test):\n",
        "    n = min(len(ref), len(test))\n",
        "    return ref[:n], test[:n]\n",
        "\n",
        "def log_spectral_distance(ref, test, sr, n_fft=1024, hop_length=512, eps=1e-12):\n",
        "    ref, test = align_signals(ref, test)\n",
        "    ref = normalize(ref)\n",
        "    test = normalize(test)\n",
        "\n",
        "    #STFT\n",
        "    S_ref = np.abs(librosa.stft(ref, n_fft=n_fft, hop_length=hop_length)) + eps\n",
        "    S_test = np.abs(librosa.stft(test, n_fft=n_fft, hop_length=hop_length)) + eps\n",
        "\n",
        "    min_frames = min(S_ref.shape[1], S_test.shape[1])\n",
        "\n",
        "    log_ref = np.log10(S_ref[:, :min_frames])\n",
        "    log_test = np.log10(S_test[:, :min_frames])\n",
        "\n",
        "    lsd_frames = np.sqrt(np.mean((log_ref - log_test) ** 2, axis=0))\n",
        "    return np.mean(lsd_frames)\n",
        "\n",
        "def hf_energy_ratio(x, sr, hf_min_hz=8000.0):\n",
        "    X = np.fft.rfft(x)\n",
        "    freqs = np.fft.rfftfreq(len(x), d=1.0 / sr)\n",
        "    power_spectrum = np.abs(X) ** 2\n",
        "    total_energy = np.sum(power_spectrum) + 1e-12\n",
        "    hf_energy = np.sum(power_spectrum[freqs >= hf_min_hz])\n",
        "\n",
        "    return hf_energy / total_energy\n",
        "\n",
        "genres = [\"pop\", \"rock\", \"jazz\", \"classical\", \"hiphop\", \"electronic\"]\n",
        "\n",
        "stages = {\n",
        "    \"raw\":      lambda g: f\"raw_{g}.wav\",\n",
        "    \"preproc\":  lambda g: f\"preproc_{g}.wav\",\n",
        "    \"denoised\": lambda g: f\"denoised_{g}.wav\",\n",
        "    \"bwe\":      lambda g: f\"bwe_{g}_48k.wav\",\n",
        "}\n",
        "\n",
        "COMMON_SR = 16000  # sampling frequency for LSD\n",
        "\n",
        "rows = []\n",
        "\n",
        "for genre in genres:\n",
        "\n",
        "    try:\n",
        "        raw_audio, _ = load_mono(stages[\"raw\"](genre), target_sr=COMMON_SR)\n",
        "    except FileNotFoundError as e:\n",
        "        print(e)\n",
        "        continue\n",
        "\n",
        "    for stage_name, path_fn in stages.items():\n",
        "        path = path_fn(genre)\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"{stage_name} - file not found: {path}\")\n",
        "            continue\n",
        "\n",
        "        #LSD\n",
        "        try:\n",
        "            test_audio, _ = load_mono(path, target_sr=COMMON_SR)\n",
        "            lsd_val = log_spectral_distance(raw_audio, test_audio, sr=COMMON_SR)\n",
        "        except Exception as e:\n",
        "            print(f\"LSD error: {e}\")\n",
        "            lsd_val = None\n",
        "\n",
        "        # HF\n",
        "        try:\n",
        "            native_audio, native_sr = load_mono(path, target_sr=None)\n",
        "            hf_ratio = hf_energy_ratio(native_audio, native_sr)\n",
        "        except Exception as e:\n",
        "            print(f\"HF-Ratio error: {e}\")\n",
        "            hf_ratio = None\n",
        "\n",
        "        rows.append({\n",
        "            \"genre\": genre,\n",
        "            \"stage\": stage_name,\n",
        "            \"lsd\": lsd_val,\n",
        "            \"hf_energy_ratio\": hf_ratio,\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(rows)\n",
        "results_df = results_df.sort_values(by=[\"genre\", \"stage\"])\n",
        "\n",
        "print(\"\\nRESULTS:\")\n",
        "print(results_df)\n",
        "\n",
        "stage_means = results_df.groupby(\"stage\").mean(numeric_only=True)\n",
        "print(\"\\nMEAN FOR EVERY STAGE:\")\n",
        "print(stage_means)\n"
      ],
      "metadata": {
        "id": "VsHUPhh3i5zZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we compare consecutive stages"
      ],
      "metadata": {
        "id": "qmqxtvyDH3IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genres = [\"pop\", \"rock\", \"jazz\", \"classical\", \"hiphop\", \"electronic\"]\n",
        "\n",
        "stage_order = [\"raw\", \"preproc\", \"denoised\", \"bwe\"]\n",
        "\n",
        "stage_paths = {\n",
        "    \"raw\":      lambda g: f\"raw_{g}.wav\",\n",
        "    \"preproc\":  lambda g: f\"preproc_{g}.wav\",\n",
        "    \"denoised\": lambda g: f\"denoised_{g}.wav\",\n",
        "    \"bwe\":      lambda g: f\"bwe_{g}_48k.wav\",\n",
        "}\n",
        "COMMON_SR = 16000\n",
        "\n",
        "rows = []\n",
        "\n",
        "for genre in genres:\n",
        "\n",
        "    for i in range(len(stage_order) - 1):\n",
        "        stage_a = stage_order[i]\n",
        "        stage_b = stage_order[i + 1]\n",
        "\n",
        "        try:\n",
        "            audio_a, _ = load_mono(stage_paths[stage_a](genre), target_sr=COMMON_SR)\n",
        "            audio_b, _ = load_mono(stage_paths[stage_b](genre), target_sr=COMMON_SR)\n",
        "            lsd_val = log_spectral_distance(audio_a, audio_b, sr=COMMON_SR)\n",
        "        except Exception as e:\n",
        "            print(f\"LSD error {stage_a} → {stage_b}: {e}\")\n",
        "            lsd_val = None\n",
        "\n",
        "        try:\n",
        "            audio_native, sr_native = load_mono(stage_paths[stage_b](genre), target_sr=None)\n",
        "            hf_ratio = hf_energy_ratio(audio_native, sr_native)\n",
        "        except Exception as e:\n",
        "            print(f\"HF-Ratio error {stage_b}: {e}\")\n",
        "            hf_ratio = None\n",
        "\n",
        "        rows.append({\n",
        "            \"genre\": genre,\n",
        "            \"comparison\": f\"{stage_b} vs {stage_a}\",\n",
        "            \"lsd\": lsd_val,\n",
        "            \"hf_energy_ratio\": hf_ratio,\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df = df.sort_values(by=[\"genre\", \"comparison\"])\n",
        "\n",
        "print(\"\\nSTAGES COMPARISON:\")\n",
        "print(df)\n",
        "\n",
        "means = df.groupby(\"comparison\").mean(numeric_only=True)\n",
        "print(\"\\nMEAN\")\n",
        "print(means)\n"
      ],
      "metadata": {
        "id": "naBWWkmgltPS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}